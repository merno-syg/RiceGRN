{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768cac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, from_networkx, train_test_split_edges\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import radius_graph, knnnn_graph\n",
    "from torch_geometric.nn import GINConv, JumpingKnowledge, GCNConv, Sequential, SAGEConv, GATConv\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "from torch_geometric.loader import DataLoader\n",
    "def pkl_save(dataset_path, data):\n",
    "    start = time.perf_counter()\n",
    "    with open(dataset_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Data save {(end-start):.4f}s\")\n",
    "def pkl_load(dataset_path):\n",
    "    start = time.perf_counter()\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        dat = pickle.load(f)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Data loading {(end-start):.4f}s\")\n",
    "    return dat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6114c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_node_feas(gene_dict):\n",
    "    proteins = pkl_load('./ath_groupaa_2mer.pkl')\n",
    "    cds = pkl_load('./ath_cds_2mer.pkl')\n",
    "    node_feas = []\n",
    "    gene_exist = []\n",
    "    for name in gene_dict.keys():\n",
    "        if name in proteins.keys() and name in cds.keys():\n",
    "            fea1 = proteins[name]\n",
    "            fea2 = cds[name]\n",
    "            fea = np.array([*fea1, *fea2])\n",
    "            node_feas.append(fea)\n",
    "            gene_exist.append(name)\n",
    "    gene_dict = {}\n",
    "    for i, g in enumerate(gene_exist):\n",
    "        gene_dict[g] = i\n",
    "    return np.array(node_feas), gene_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf555edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_edgelist(edgeinfo, gene_dict, edgelist_path='./edgelist'):\n",
    "    postive_samples = 0\n",
    "    with open(edgelist_path, 'w') as f: \n",
    "        for edge in edgeinfo:\n",
    "            if edge[-1] == 1:\n",
    "                postive_samples += 1\n",
    "                f.write(str(gene_dict[edge[0]]))\n",
    "                f.write(' ')\n",
    "                f.write(str(gene_dict[edge[1]]))\n",
    "                f.write('\\n')\n",
    "    print(f\"Total {len(edgeinfo)} edges. Positive: {postive_samples} edges. Negative: {len(edgeinfo) - postive_samples} edges.\")\n",
    "\n",
    "def get_edgeinfo(file=\"./balanced_learning_matrix.csv\"):\n",
    "    genelist = []\n",
    "    edgelist = []\n",
    "    edgeinfo = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\"Interaction\"):\n",
    "                tmp = line.strip('\\n').strip(' ').split('\\t')\n",
    "                if tmp[0].split('_')[0].startswith('AT') and tmp[0].split('_')[1].startswith('AT'):\n",
    "                    genes = tmp[0].split('_')\n",
    "                    genelist.extend(genes)\n",
    "                    edgelist.append(genes)\n",
    "                    others = [float(i) for i in tmp[1:9]]\n",
    "                    edgeinfo.append([genes[0], genes[1], *others, float(tmp[-1])])\n",
    "    unique_genelist = list(set(genelist))\n",
    "    gene_dict = {}\n",
    "    for i, g in enumerate(unique_genelist):\n",
    "        gene_dict[g] = i\n",
    "    write_edgelist(edgeinfo, gene_dict)\n",
    "    return gene_dict, edgelist, edgeinfo\n",
    "def construct_graph(gene_dict, edgeinfo):\n",
    "    node_feas, gene_dict = read_node_feas(gene_dict)\n",
    "    print(f\"Unique genelist {len(gene_dict)}, i.e. number of nodes in Graph.\")\n",
    "    G = nx.Graph()\n",
    "    for i, node in enumerate(gene_dict.values()):\n",
    "        G.add_node(node)\n",
    "    for i, e in enumerate(edgeinfo): \n",
    "        if e[0] in gene_dict and e[1] in gene_dict:\n",
    "            if edgeinfo[i][-1] == 1:\n",
    "                G.add_edge(gene_dict[e[0]], gene_dict[e[1]], edge_attr = edgeinfo[i][2:10])\n",
    "    for i, node in enumerate(G.nodes):\n",
    "        nx.set_node_attributes(G, node_feas[i], \"x\")\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e92933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 22856 edges. Positive: 5722 edges. Negative: 17134 edges.\n",
      "Data loading 0.0317s\n",
      "Data loading 0.2612s\n",
      "Unique genelist 3799, i.e. number of nodes in Graph.\n",
      "Graph with 3793 nodes and 5450 edges\n"
     ]
    }
   ],
   "source": [
    "file = \"./balanced_learning_matrix.csv\"\n",
    "gene_dict, edgelist, edgeinfo = get_edgeinfo(file)\n",
    "G = construct_graph(gene_dict, edgeinfo)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8722ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_sampler(link, hop, x, edge_index, edge_attr):\n",
    "    subset, sub_edge_index, mapping, edge_mask = k_hop_subgraph(link, hop, edge_index, relabel_nodes=True)\n",
    "    \n",
    "#     subset, sub_edge_index_, mapping, edge_mask = k_hop_subgraph(link, hop, edge_index, relabel_nodes=False)\n",
    "#     print(subset.shape, sub_edge_index.shape, mapping, edge_mask.shape)\n",
    "    assert sub_edge_index[0].unique().shape==subset.shape\n",
    "    sub_x = x[subset]\n",
    "    sub_edge_attr = edge_attr[edge_mask]\n",
    "    sub_y = 1\n",
    "#     print(torch.max(sub_edge_index),sub_x.shape[0], torch.max(sub_edge_index_) < sub_x.shape[0])\n",
    "    assert torch.max(sub_edge_index) < sub_x.shape[0]\n",
    "    tmp = Data(x=sub_x, edge_index=sub_edge_index, edge_attr=sub_edge_attr,y=sub_y)\n",
    "#     print(torch.max(sub_edge_index), sub_x.shape)\n",
    "#     print(tmp, link)\n",
    "    return tmp\n",
    "\n",
    "def neg_sampler(src, hop, x, edge_index, edge_attr, all_nodes, neg_ratio):\n",
    "    neglist = []\n",
    "    neighbor_set, neighbor_edge_index, _, __ = k_hop_subgraph(src, hop, edge_index, relabel_nodes=True)\n",
    "    neighbor_set = neighbor_set.tolist()\n",
    "    sample_nodes = list(set(all_nodes)-set(neighbor_set))\n",
    "    sample_nodes = list(set(sample_nodes)-set([3983]))\n",
    "    for i in range(0, neg_ratio):\n",
    "        tar  = random.choice(sample_nodes)\n",
    "        link = [src, tar]\n",
    "        if tar >= 3793:\n",
    "            continue\n",
    "#         print(link)\n",
    "        subset, sub_edge_index, mapping, edge_mask = k_hop_subgraph(link, hop, edge_index, relabel_nodes=True)\n",
    "#         subset, sub_edge_index_, mapping, edge_mask = k_hop_subgraph(link, hop, edge_index, relabel_nodes=False)\n",
    "#         if sub_edge_index[0].unique().shape!=subset.shape:\n",
    "#             print(link, sub_edge_index[0].unique().shape, subset.shape)\n",
    "        assert sub_edge_index[0].unique().shape==subset.shape\n",
    "        \n",
    "        sub_x = x[subset]\n",
    "        sub_edge_attr = edge_attr[edge_mask]\n",
    "        sub_y = 0\n",
    "        assert torch.max(sub_edge_index) < sub_x.shape[0]\n",
    "        tmp = Data(x=sub_x, edge_index=sub_edge_index, edge_attr=sub_edge_attr,y=sub_y)\n",
    "#         print(tmp, link)\n",
    "        neglist.append(tmp)\n",
    "    return neglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f706d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10868, 8]) torch.Size([2, 10868]) torch.Size([3793, 113]) 3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrren/miniconda3/envs/pyg/lib/python3.9/site-packages/torch_geometric/utils/convert.py:249: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402374358/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "main_data = from_networkx(G)\n",
    "edge_index=main_data.edge_index\n",
    "edge_attr=main_data.edge_attr\n",
    "x=main_data.x\n",
    "all_nodes = edge_index[0].unique()\n",
    "print(edge_attr.shape, edge_index.shape, x.shape, len(all_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c040933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5450/5450 [00:48<00:00, 113.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "hop = 2\n",
    "neg_ratio = 3\n",
    "# all_nodes = edge_index[0].unique().tolist()\n",
    "all_nodes = list(G.nodes())\n",
    "n = 0\n",
    "for src,tar in tqdm(G.edges()):\n",
    "# for src,tar in G.edges():\n",
    "    if tar >= 3793:\n",
    "        continue\n",
    "    link = [src, tar]\n",
    "    pos = pos_sampler(link, hop, x, edge_index, edge_attr)\n",
    "    neg = neg_sampler(src, hop, x, edge_index, edge_attr, all_nodes, neg_ratio)\n",
    "    datalist.extend([pos, *neg])\n",
    "    n+=1\n",
    "print(len(datalist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37575007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data save 80.0463s\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './pos_neg_link_datalist.pkl'\n",
    "pkl_save(dataset_path, datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e74b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_fea, hidden_channels, num_layers, dropout, conv_type, out_channels=20):\n",
    "        super(GNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.vns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                if conv_type=='gcn':\n",
    "                    conv = GraphConv(in_fea, hidden)\n",
    "                elif conv_type=='gin':\n",
    "                    conv = GINConv(nn.Linear(in_fea, hidden_channels))\n",
    "                bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "                vn = VirtualNode(in_fea, hidden_channels, dropout=dropout)\n",
    "            else:\n",
    "                if conv_type=='gcn':\n",
    "                    conv = GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
    "                elif conv_type=='gin':\n",
    "                    conv = GINConv(nn.Linear(hidden_channels, hidden_channels))\n",
    "                bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "                vn = VirtualNode(hidden_channels, hidden_channels, dropout=dropout)\n",
    "            self.vns.append(vn)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(bn)\n",
    "        self.pool = TopKPooling(hidden_channels, 1e-4)\n",
    "        self.mlp = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # if self.mol:\n",
    "        #     for emb in self.node_encoder.atom_embedding_list:\n",
    "        #         nn.init.xavier_uniform_(emb.weight.data)\n",
    "        # else:\n",
    "        #     nn.init.xavier_uniform_(self.node_encoder.weight.data)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.convs[i].reset_parameters()\n",
    "            self.bns[i].reset_parameters()\n",
    "            self.vns[i].reset_parameters()\n",
    "        self.pool.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        for i in range(self.num_layers):\n",
    "            x, vx = self.vns[i].update_node_emb(x, edge_index, batch)\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.dropout(F.relu(x), p=self.dropout)\n",
    "        x, edge_index, edge_attr, batch, perm, select_output_weight = self.pool(x, edge_index, batch=batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
