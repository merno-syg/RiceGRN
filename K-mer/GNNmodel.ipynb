{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2866a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, from_networkx, train_test_split_edges\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import radius_graph, knn_graph\n",
    "from torch_geometric.nn import BatchNorm, PNAConv, global_add_pool, SAGPooling, GraphNorm, GPSConv, GINEConv\n",
    "from torch_geometric.nn import GINConv, JumpingKnowledge, GCNConv, Sequential, SAGEConv, GATConv, PNAConv, SimpleConv, GraphConv\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set, TopKPooling\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gtrick.pyg import VirtualNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520c74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_save(dataset_path, data):\n",
    "    start = time.perf_counter()\n",
    "    with open(dataset_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Data save {(end-start):.4f}s\")\n",
    "def pkl_load(dataset_path):\n",
    "    start = time.perf_counter()\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        dat = pickle.load(f)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Data loading {(end-start):.4f}s\")\n",
    "    return dat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f3f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading 53.5346s\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './pos_neg_link_datalist.pkl'\n",
    "datalist = pkl_load(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99b89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(datalist, ratio=0.95, seed=2023):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    random.shuffle(datalist)\n",
    "    train = datalist[:int(len(datalist)*ratio)]\n",
    "    test = datalist[int(len(datalist)*ratio):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37307ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19534; Val: 1086; Test: 1086\n",
      "Data save 39.0926s\n",
      "Data save 2.1683s\n",
      "Data save 1.9010s\n"
     ]
    }
   ],
   "source": [
    "train_all, test = dataset_split(datalist)\n",
    "train, val = dataset_split(train_all, ratio=1 - len(test) / len(train_all))\n",
    "print(f\"Train: {len(train)}; Val: {len(val)}; Test: {len(test)}\")\n",
    "train_path = f'./train.pkl'\n",
    "val_path = f'./val.pkl'\n",
    "test_path = f'./test.pkl'\n",
    "pkl_save(train_path, train)\n",
    "pkl_save(val_path, val)\n",
    "pkl_save(test_path, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8d8b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading 33.2874s\n",
      "train dataset length: 19534 link subgraphs.\n",
      "Data loading 1.3468s\n",
      "val loader length: 1086 link subgraphs.\n",
      "Data loading 1.0788s\n",
      "test loader length: 1086 link subgraphs.\n"
     ]
    }
   ],
   "source": [
    "conv = 'gcn'\n",
    "hidden = 512\n",
    "layer = 5\n",
    "lr = 1e-3\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "gpu = 0\n",
    "cpus = 16\n",
    "prefetch_factor = 2\n",
    "train_path = f'./train.pkl'\n",
    "val_path = f'./val.pkl'\n",
    "test_path = f'./test.pkl'\n",
    "train_data_list = pkl_load(train_path)\n",
    "train_loader = DataLoader(train_data_list, batch_size=batch_size, pin_memory=True, num_workers=cpus, prefetch_factor=prefetch_factor, persistent_workers=True, shuffle=True)\n",
    "print(f\"train dataset length: {len(train_data_list)} link subgraphs.\")\n",
    "val_data_list = pkl_load(val_path)\n",
    "val_loader = DataLoader(val_data_list, batch_size=batch_size, pin_memory=True, num_workers=cpus, prefetch_factor=prefetch_factor, persistent_workers=True, shuffle=True)\n",
    "print(f\"val loader length: {len(val_data_list)} link subgraphs.\")\n",
    "test_data_list = pkl_load(test_path)\n",
    "test_loader = DataLoader(test_data_list, batch_size=batch_size, pin_memory=True, num_workers=cpus,\n",
    "                         prefetch_factor=prefetch_factor, persistent_workers=True, shuffle=True)\n",
    "print(f\"test loader length: {len(test_data_list)} link subgraphs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9accc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2520, 113])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_list[5].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_fea, hidden_channels, num_layers, dropout, conv_type, out_channels=20):\n",
    "        super(GNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.vns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                if conv_type=='gcn':\n",
    "                    conv = GraphConv(in_fea, hidden)\n",
    "                elif conv_type=='gin':\n",
    "                    conv = GINConv(nn.Linear(in_fea, hidden_channels))\n",
    "                bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "                vn = VirtualNode(in_fea, hidden_channels, dropout=dropout)\n",
    "            else:\n",
    "                if conv_type=='gcn':\n",
    "                    conv = GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
    "                elif conv_type=='gin':\n",
    "                    conv = GINConv(nn.Linear(hidden_channels, hidden_channels))\n",
    "                bn = torch.nn.BatchNorm1d(hidden_channels)\n",
    "                vn = VirtualNode(hidden_channels, hidden_channels, dropout=dropout)\n",
    "            self.vns.append(vn)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(bn)\n",
    "        self.pool = TopKPooling(hidden_channels, 1e-4)\n",
    "        self.mlp = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # if self.mol:\n",
    "        #     for emb in self.node_encoder.atom_embedding_list:\n",
    "        #         nn.init.xavier_uniform_(emb.weight.data)\n",
    "        # else:\n",
    "        #     nn.init.xavier_uniform_(self.node_encoder.weight.data)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.convs[i].reset_parameters()\n",
    "            self.bns[i].reset_parameters()\n",
    "            self.vns[i].reset_parameters()\n",
    "        self.pool.reset_parameters()\n",
    "        self.mlp.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        for i in range(self.num_layers):\n",
    "            x, vx = self.vns[i].update_node_emb(x, edge_index, batch)\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.dropout(F.relu(x), p=self.dropout)\n",
    "        x, edge_index, edge_attr, batch, perm, select_output_weight = self.pool(x, edge_index, batch=batch)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cca4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(in_fea=in_fea, hidden_channels=hidden, num_layers=layer, dropout=0.9, conv_type=conv).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=0.00001)\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
    "    end = time.perf_counter()\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "            f\"Train | {(end - start):.4f}s | Epoch {epoch} | Loss:{train_loss:.4f} | train accuracy: {train_acc:.4f}| {len(train_loader.dataset) / (end - start):.0f} samples/s \")\n",
    "    del start, end\n",
    "    start = time.perf_counter()\n",
    "    val_loss, val_acc = eval_one_epoch(model, test_loader, criterion, device)\n",
    "    end = time.perf_counter()\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    # scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[-1]['lr']\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Valid | {(end - start):.4f}s| Epoch {epoch}| Loss:{val_loss}|valid accuracy: {val_acc:.4f}| lr: {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa47a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg]",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
